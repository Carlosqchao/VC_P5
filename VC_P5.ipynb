{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392ab9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juang\\anaconda3\\envs\\VC_Pract5\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa15027",
   "metadata": {},
   "source": [
    "EMOTION RECOGNITION MODEL - FER2013\n",
    "\n",
    "DESCRIPCIÓN GENERAL\n",
    "Este notebook entrena un modelo de Red Neuronal Convolucional (CNN) para reconocer emociones en imágenes faciales usando el dataset FER2013. El modelo es capaz de clasificar imágenes en 7 categorías de emociones diferentes.\n",
    "\n",
    "FUNCIONALIDADES PRINCIPALES\n",
    "\n",
    "1. CARGA Y PREPARACIÓN DE DATOS\n",
    "- Carga de imágenes desde directorio de entrenamiento y prueba\n",
    "- Aplicación de Data Augmentation en el conjunto de entrenamiento\n",
    "- Normalización de imágenes (escala 0-1)\n",
    "- Generadores de datos para procesamiento eficiente en lotes\n",
    "\n",
    "2. ARQUITECTURA DEL MODELO CNN\n",
    "- 4 bloques convolucionales con capas de convolución, normalización de lotes y dropout\n",
    "- Pooling progresivo para reducción de dimensionalidad\n",
    "- Capas densas (fully connected) para clasificación final\n",
    "- Softmax para salida probabilística de 7 clases\n",
    "\n",
    "3. ENTRENAMIENTO\n",
    "- Optimizador: Adam (learning rate: 0.0001)\n",
    "- Loss: Categorical Crossentropy\n",
    "- Callbacks incluyen:\n",
    "  - Early Stopping (paciencia: 10 epochs)\n",
    "  - Reducción de tasa de aprendizaje (ReduceLROnPlateau)\n",
    "  - Guardado del mejor modelo (ModelCheckpoint)\n",
    "\n",
    "4. EMOCIONES DETECTADAS\n",
    "1. Angry (Enojado)\n",
    "2. Disgust (Asqueado)\n",
    "3. Fear (Asustado)\n",
    "4. Happy (Feliz)\n",
    "5. Neutral (Neutral)\n",
    "6. Sad (Triste)\n",
    "7. Surprise (Sorprendido)\n",
    "\n",
    "5. EVALUACIÓN Y VISUALIZACIÓN\n",
    "- Matriz de confusión\n",
    "- Reporte de clasificación (precisión, recall, f1-score)\n",
    "- Gráficos de accuracy y loss durante el entrenamiento\n",
    "- Función de predicción para imágenes individuales\n",
    "\n",
    "6. OUTPUTS\n",
    "- best_emotion_model.h5: Mejor modelo guardado durante el entrenamiento\n",
    "- final_emotion_model.h5: Modelo final después de todos los epochs\n",
    "- training_history.png: Gráficos de entrenamiento\n",
    "- confusion_matrix.png: Matriz de confusión del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed174f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'VC_P5 (Python 3.11.5)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n VC_P5 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración de rutas - ajusta según tu estructura en Kaggle\n",
    "TRAIN_DIR = '/kaggle/input/fer2013/train'\n",
    "TEST_DIR = '/kaggle/input/fer2013/test'\n",
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "# Clases de emociones\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "num_classes = len(emotions)\n",
    "\n",
    "# Data Augmentation para entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Solo normalización para validación/test\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generadores de datos\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Clases encontradas: {train_generator.class_indices}\")\n",
    "print(f\"Imágenes de entrenamiento: {train_generator.samples}\")\n",
    "print(f\"Imágenes de prueba: {test_generator.samples}\")\n",
    "\n",
    "# Construcción del modelo CNN\n",
    "def create_emotion_cnn():\n",
    "    model = keras.Sequential([\n",
    "        # Bloque 1\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 2\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 3\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 4\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Capas densas\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear y compilar el modelo\n",
    "model = create_emotion_cnn()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_emotion_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo final\n",
    "model.save('final_emotion_model.h5')\n",
    "print(\"Modelo guardado exitosamente!\")\n",
    "\n",
    "# Visualizar resultados del entrenamiento\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"\\n=== Evaluación en el conjunto de prueba ===\")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predicciones para la matriz de confusión\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, steps=len(test_generator))\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"\\n=== Reporte de Clasificación ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=emotions))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=emotions, yticklabels=emotions)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Función para predecir emociones en imágenes individuales\n",
    "def predict_emotion(image_path):\n",
    "    \"\"\"\n",
    "    Predice la emoción de una imagen individual\n",
    "    \"\"\"\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, \n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='grayscale'\n",
    "    )\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    prediction = model.predict(img_array, verbose=0)\n",
    "    emotion_idx = np.argmax(prediction)\n",
    "    confidence = prediction[0][emotion_idx]\n",
    "    \n",
    "    return emotions[emotion_idx], confidence\n",
    "\n",
    "# Ejemplo de uso de la función de predicción\n",
    "# emotion, conf = predict_emotion('path/to/image.jpg')\n",
    "# print(f\"Emoción detectada: {emotion} (Confianza: {conf:.2%})\")\n",
    "\n",
    "print(\"\\n¡Entrenamiento completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725cb849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presiona 'q' para salir de la detección\n",
      "Detección finalizada\n",
      "Detección finalizada\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar el modelo entrenado\n",
    "model = load_model('best_emotion_model.h5')\n",
    "\n",
    "# Emociones\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "emotions_spanish = ['Enojado', 'Asqueado', 'Asustado', 'Feliz', 'Neutral', 'Triste', 'Sorprendido']\n",
    "\n",
    "# Cargar el cascada de clasificador para detección de rostros\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    ")\n",
    "\n",
    "# Inicializar la webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Configurar resolución (opcional, para mejor rendimiento)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "print(\"Presiona 'q' para salir de la detección\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error al capturar la cámara\")\n",
    "        break\n",
    "    \n",
    "    # Convertir a escala de grises para detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detectar rostros\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.3,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "    \n",
    "    # Procesar cada rostro detectado\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Dibujar rectángulo alrededor del rostro\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Extraer la región del rostro en escala de grises\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        \n",
    "        # Redimensionar a 48x48 (tamaño del modelo)\n",
    "        roi_resized = cv2.resize(roi_gray, (48, 48))\n",
    "        \n",
    "        # Normalizar\n",
    "        roi_normalized = roi_resized / 255.0\n",
    "        \n",
    "        # Agregar dimensión de canal\n",
    "        roi_input = np.expand_dims(roi_normalized, axis=-1)\n",
    "        roi_input = np.expand_dims(roi_input, axis=0)\n",
    "        \n",
    "        # Realizar predicción\n",
    "        prediction = model.predict(roi_input, verbose=0)\n",
    "        emotion_idx = np.argmax(prediction)\n",
    "        confidence = prediction[0][emotion_idx]\n",
    "        \n",
    "        emotion = emotions_spanish[emotion_idx]\n",
    "        \n",
    "        # Mostrar la emoción y confianza\n",
    "        text = f\"{emotion}: {confidence:.2%}\"\n",
    "        cv2.putText(frame, text, (x, y - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    \n",
    "    # Mostrar el frame\n",
    "    cv2.imshow('Detección de Emociones', frame)\n",
    "    \n",
    "    # Presionar 'q' para salir\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Detección finalizada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_Pract5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
