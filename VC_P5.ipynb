{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392ab9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa15027",
   "metadata": {},
   "source": [
    "EMOTION RECOGNITION MODEL - FER2013\n",
    "\n",
    "DESCRIPCIÓN GENERAL\n",
    "Este notebook entrena un modelo de Red Neuronal Convolucional (CNN) para reconocer emociones en imágenes faciales usando el dataset FER2013. El modelo es capaz de clasificar imágenes en 7 categorías de emociones diferentes.\n",
    "\n",
    "FUNCIONALIDADES PRINCIPALES\n",
    "\n",
    "1. CARGA Y PREPARACIÓN DE DATOS\n",
    "- Carga de imágenes desde directorio de entrenamiento y prueba\n",
    "- Aplicación de Data Augmentation en el conjunto de entrenamiento\n",
    "- Normalización de imágenes (escala 0-1)\n",
    "- Generadores de datos para procesamiento eficiente en lotes\n",
    "\n",
    "2. ARQUITECTURA DEL MODELO CNN\n",
    "- 4 bloques convolucionales con capas de convolución, normalización de lotes y dropout\n",
    "- Pooling progresivo para reducción de dimensionalidad\n",
    "- Capas densas (fully connected) para clasificación final\n",
    "- Softmax para salida probabilística de 7 clases\n",
    "\n",
    "3. ENTRENAMIENTO\n",
    "- Optimizador: Adam (learning rate: 0.0001)\n",
    "- Loss: Categorical Crossentropy\n",
    "- Callbacks incluyen:\n",
    "  - Early Stopping (paciencia: 10 epochs)\n",
    "  - Reducción de tasa de aprendizaje (ReduceLROnPlateau)\n",
    "  - Guardado del mejor modelo (ModelCheckpoint)\n",
    "\n",
    "4. EMOCIONES DETECTADAS\n",
    "1. Angry (Cabreado)\n",
    "2. Disgust (Asqueado)\n",
    "3. Fear (Asustado)\n",
    "4. Happy (Feliz)\n",
    "5. Neutral (Neutral)\n",
    "6. Sad (Triste)\n",
    "7. Surprise (Sorprendido)\n",
    "\n",
    "5. EVALUACIÓN Y VISUALIZACIÓN\n",
    "- Matriz de confusión\n",
    "- Reporte de clasificación (precisión, recall, f1-score)\n",
    "- Gráficos de accuracy y loss durante el entrenamiento\n",
    "- Función de predicción para imágenes individuales\n",
    "\n",
    "6. OUTPUTS\n",
    "- best_emotion_model.h5: Mejor modelo guardado durante el entrenamiento\n",
    "- final_emotion_model.h5: Modelo final después de todos los epochs\n",
    "- training_history.png: Gráficos de entrenamiento\n",
    "- confusion_matrix.png: Matriz de confusión del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed174f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'VC_P5 (Python 3.11.5)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n VC_P5 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración de rutas - ajusta según tu estructura en Kaggle\n",
    "TRAIN_DIR = '/kaggle/input/fer2013/train'\n",
    "TEST_DIR = '/kaggle/input/fer2013/test'\n",
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "# Clases de emociones\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "num_classes = len(emotions)\n",
    "\n",
    "# Data Augmentation para entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Solo normalización para validación/test\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generadores de datos\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Clases encontradas: {train_generator.class_indices}\")\n",
    "print(f\"Imágenes de entrenamiento: {train_generator.samples}\")\n",
    "print(f\"Imágenes de prueba: {test_generator.samples}\")\n",
    "\n",
    "# Construcción del modelo CNN\n",
    "def create_emotion_cnn():\n",
    "    model = keras.Sequential([\n",
    "        # Bloque 1\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 2\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 3\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloque 4\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Capas densas\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear y compilar el modelo\n",
    "model = create_emotion_cnn()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_emotion_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo final\n",
    "model.save('final_emotion_model.h5')\n",
    "print(\"Modelo guardado exitosamente!\")\n",
    "\n",
    "# Visualizar resultados del entrenamiento\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"\\n=== Evaluación en el conjunto de prueba ===\")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predicciones para la matriz de confusión\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, steps=len(test_generator))\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"\\n=== Reporte de Clasificación ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=emotions))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=emotions, yticklabels=emotions)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Función para predecir emociones en imágenes individuales\n",
    "def predict_emotion(image_path):\n",
    "    \"\"\"\n",
    "    Predice la emoción de una imagen individual\n",
    "    \"\"\"\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, \n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='grayscale'\n",
    "    )\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    prediction = model.predict(img_array, verbose=0)\n",
    "    emotion_idx = np.argmax(prediction)\n",
    "    confidence = prediction[0][emotion_idx]\n",
    "    \n",
    "    return emotions[emotion_idx], confidence\n",
    "\n",
    "# Ejemplo de uso de la función de predicción\n",
    "# emotion, conf = predict_emotion('path/to/image.jpg')\n",
    "# print(f\"Emoción detectada: {emotion} (Confianza: {conf:.2%})\")\n",
    "\n",
    "print(\"\\n¡Entrenamiento completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "725cb849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventana abierta. Presiona 'q' para salir.\n",
      "Las emociones cambiarán el color y los efectos visuales\n",
      "\n",
      "Deteccion finalizada\n",
      "Sistema de detección de emociones finalizado\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "model = load_model('best_emotion_model.h5')\n",
    "\n",
    "# Emociones\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "emotions_spanish = ['Cabreado', 'Asqueado', 'Asustado', 'Feliz', 'Neutral', 'Triste', 'Sorprendido']\n",
    "\n",
    "# Archivos PNG (ajusta nombres/rutas si es necesario)\n",
    "png_files = {\n",
    "    0: 'angry.jpg',    # angry -> Cabreado\n",
    "    1: 'disgust.png',   # disgust -> Berto serio\n",
    "    2: 'fear.png',      # fear -> Asustado\n",
    "    3: 'happy.png',   # happy -> moguefeliz\n",
    "    4: 'neutral.png',    # neutral -> Neutral\n",
    "    5: 'sadness.png',       # sad -> Triste\n",
    "    6: 'surprise.png'  # surprise -> Sorprendido\n",
    "}\n",
    "\n",
    "# Cargar las imágenes PNG con canal alfa (si existen)\n",
    "png_images = {}\n",
    "for idx, fname in png_files.items():\n",
    "    if os.path.exists(fname):\n",
    "        img = cv2.imread(fname, cv2.IMREAD_UNCHANGED)  # mantiene alfa\n",
    "        if img is None:\n",
    "            print(f\"Advertencia: no se pudo leer {fname}\")\n",
    "        else:\n",
    "            png_images[idx] = img\n",
    "    else:\n",
    "        print(f\"Advertencia: archivo PNG no encontrado: {fname}\")\n",
    "\n",
    "# Definir colores y efectos por emoción (BGR)\n",
    "emotion_config = {\n",
    "    0: {'color': (0, 0, 255), 'name': 'Cabreado'},      # Rojo\n",
    "    1: {'color': (0, 165, 255), 'name': 'Asqueado'},    # Naranja\n",
    "    2: {'color': (255, 0, 0), 'name': 'Asustado'},      # Azul\n",
    "    3: {'color': (0, 255, 0), 'name': 'Feliz'},         # Verde\n",
    "    4: {'color': (128, 128, 128), 'name': 'Neutral'},   # Gris\n",
    "    5: {'color': (255, 0, 255), 'name': 'Triste'},      # Magenta\n",
    "    6: {'color': (0, 255, 255), 'name': 'Sorprendido'}  # Amarillo\n",
    "}\n",
    "\n",
    "# Cargar el cascada de clasificador para detección de rostros\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    ")\n",
    "\n",
    "# Variable de control\n",
    "stop_event = threading.Event()\n",
    "\n",
    "\n",
    "def overlay_png_on_frame(frame, png, x, y, w, h, scale=1.0):\n",
    "    \"\"\"Redimensiona y superpone `png` sobre `frame` centrado en la cara (x,y,w,h).\n",
    "       `png` puede tener 3 o 4 canales (BGRA).\"\"\"\n",
    "    if png is None:\n",
    "        return frame\n",
    "\n",
    "    # Calcular tamaño objetivo (un poco más grande que la cara)\n",
    "    target_w = int(w * scale)\n",
    "    target_h = int(h * scale)\n",
    "\n",
    "    # Redimensionar la PNG manteniendo proporción\n",
    "    png_h, png_w = png.shape[:2]\n",
    "    if png_w == 0 or png_h == 0:\n",
    "        return frame\n",
    "\n",
    "    resized = cv2.resize(png, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Coordenadas para centrar la imagen sobre la cara\n",
    "    cx = x + w // 2\n",
    "    cy = y + h // 2\n",
    "    x1 = int(cx - target_w // 2)\n",
    "    y1 = int(cy - target_h // 2)\n",
    "    x2 = x1 + target_w\n",
    "    y2 = y1 + target_h\n",
    "\n",
    "    # Recortar coordenadas que salgan del frame\n",
    "    fh, fw = frame.shape[:2]\n",
    "    ox1, oy1 = max(0, x1), max(0, y1)\n",
    "    ox2, oy2 = min(fw, x2), min(fh, y2)\n",
    "\n",
    "    if ox1 >= ox2 or oy1 >= oy2:\n",
    "        return frame\n",
    "\n",
    "    # Coordenadas relativas dentro de la imagen redimensionada\n",
    "    rx1 = ox1 - x1\n",
    "    ry1 = oy1 - y1\n",
    "    rx2 = rx1 + (ox2 - ox1)\n",
    "    ry2 = ry1 + (oy2 - oy1)\n",
    "\n",
    "    # Extraer regiones\n",
    "    roi_frame = frame[oy1:oy2, ox1:ox2]\n",
    "    roi_png = resized[ry1:ry2, rx1:rx2]\n",
    "\n",
    "    # Si la PNG tiene canal alfa, usarlo para mezclar\n",
    "    if roi_png.shape[2] == 4:\n",
    "        b, g, r, a = cv2.split(roi_png)\n",
    "        alpha = a.astype(float) / 255.0\n",
    "        alpha = cv2.merge([alpha, alpha, alpha])\n",
    "        foreground = cv2.merge([b, g, r]).astype(float)\n",
    "        background = roi_frame.astype(float)\n",
    "        blended = cv2.convertScaleAbs(foreground * alpha + background * (1 - alpha))\n",
    "    else:\n",
    "        # PNG sin alfa: reemplazar directamente (o usar transparencia fija)\n",
    "        blended = roi_png[:, :, :3]\n",
    "\n",
    "    frame[oy1:oy2, ox1:ox2] = blended\n",
    "    return frame\n",
    "\n",
    "\n",
    "def add_emotion_effects(frame, emotion_idx, x, y, w, h, confidence):\n",
    "    \"\"\"Agrega efectos visuales según la emoción. Si existe una PNG para la emoción,\n",
    "       la superpone sobre la cara; en caso contrario, dibuja los efectos previos.\"\"\"\n",
    "    config = emotion_config.get(emotion_idx, {'color': (255, 255, 255), 'name': str(emotion_idx)})\n",
    "\n",
    "    # Si tenemos una PNG para esta emoción, superponerla (con un poco de escala)\n",
    "    png = png_images.get(emotion_idx)\n",
    "    if png is not None:\n",
    "        # Escalas recomendadas: caras pequeñas -> ampliar un poco\n",
    "        scale = 1.2\n",
    "        frame = overlay_png_on_frame(frame, png, x, y, w, h, scale=scale)\n",
    "\n",
    "        # También dibujar texto con nombre y confianza\n",
    "        text = f\"{config['name']}: {confidence:.2%}\"\n",
    "        cv2.putText(frame, text, (x + 5, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        return frame\n",
    "\n",
    "    # Fallback: efectos originales si no hay PNG\n",
    "    color = config['color']\n",
    "\n",
    "    # 1. Cambiar el color del borde del rectángulo\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "\n",
    "    # 2. Agregar fondo de color en la esquina superior\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (x - 5, y - 50), (x + 200, y - 10), color, -1)\n",
    "    cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n",
    "\n",
    "    # 3. Mostrar texto con emoción\n",
    "    text = f\"{config['name']}: {confidence:.2%}\"\n",
    "    cv2.putText(frame, text, (x + 5, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    # 4. Agregar círculos decorativos alrededor del rostro\n",
    "    cv2.circle(frame, (x + w//2, y + h//2), w//2 + 10, color, 2)\n",
    "\n",
    "    # 5. Agregar efectos de partículas en las esquinas del rostro\n",
    "    for corner_x, corner_y in [(x, y), (x+w, y), (x, y+h), (x+w, y+h)]:\n",
    "        cv2.circle(frame, (corner_x, corner_y), 5, color, -1)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def emotion_detection_thread():\n",
    "    \"\"\"Función que ejecuta la detección de emociones en un hilo separado\"\"\"\n",
    "\n",
    "    # Inicializar la webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Configurar resolución\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    print(\"Ventana abierta. Presiona 'q' para salir.\")\n",
    "    print(\"Las emociones cambiarán el color y los efectos visuales\\n\")\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Error al capturar la cámara\")\n",
    "            break\n",
    "\n",
    "        # Convertir a escala de grises para detección de rostros\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detectar rostros\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.3,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "\n",
    "        # Procesar cada rostro detectado\n",
    "        emotion_detected = None\n",
    "        max_confidence = 0\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extraer la región del rostro en escala de grises\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "\n",
    "            # Redimensionar a 48x48 (tamaño del modelo)\n",
    "            roi_resized = cv2.resize(roi_gray, (48, 48))\n",
    "\n",
    "            # Normalizar\n",
    "            roi_normalized = roi_resized / 255.0\n",
    "\n",
    "            # Agregar dimensión de canal\n",
    "            roi_input = np.expand_dims(roi_normalized, axis=-1)\n",
    "            roi_input = np.expand_dims(roi_input, axis=0)\n",
    "\n",
    "            # Realizar predicción\n",
    "            prediction = model.predict(roi_input, verbose=0)\n",
    "            emotion_idx = np.argmax(prediction)\n",
    "            confidence = prediction[0][emotion_idx]\n",
    "\n",
    "            # Guardar la emoción más confiable\n",
    "            if confidence > max_confidence:\n",
    "                max_confidence = confidence\n",
    "                emotion_detected = emotion_idx\n",
    "\n",
    "            # Agregar efectos según la emoción\n",
    "            frame = add_emotion_effects(frame, emotion_idx, x, y, w, h, confidence)\n",
    "\n",
    "        # Agregar información general en la parte superior\n",
    "        if emotion_detected is not None:\n",
    "            config = emotion_config[emotion_detected]\n",
    "            info_text = f\"Emocion Principal: {config['name']}\"\n",
    "            cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, config['color'], 2)\n",
    "\n",
    "            # Cambiar el color de fondo del marco general según emoción\n",
    "            overlay = frame.copy()\n",
    "            color = config['color']\n",
    "            cv2.rectangle(overlay, (0, 0), (frame.shape[1], 60), color, -1)\n",
    "            cv2.addWeighted(overlay, 0.1, frame, 0.9, 0, frame)\n",
    "\n",
    "        # Mostrar el frame\n",
    "        cv2.imshow('Deteccion de Emociones', frame)\n",
    "\n",
    "        # Presionar 'q' para salir\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            stop_event.set()\n",
    "            break\n",
    "\n",
    "    # Liberar recursos\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Deteccion finalizada\")\n",
    "\n",
    "# Crear e iniciar el hilo\n",
    "thread = threading.Thread(target=emotion_detection_thread, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "# Esperar a que el hilo termine\n",
    "thread.join(timeout=300)  # Timeout de 5 minutos\n",
    "\n",
    "print(\"Sistema de detección de emociones finalizado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
