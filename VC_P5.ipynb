{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b626373",
   "metadata": {},
   "source": [
    "### Importación de Librerías Necesarias\n",
    "\n",
    "La **Celda 2** importa todas las librerías esenciales para el proyecto de reconocimiento de emociones:\n",
    "\n",
    "#### Librerías de Análisis de Datos\n",
    "- **numpy**: Operaciones numéricas y manipulación de arrays\n",
    "- **matplotlib.pyplot**: Visualización de gráficos\n",
    "- **seaborn**: Mejora de visualizaciones con temas y estilos\n",
    "\n",
    "#### Librerías de Machine Learning\n",
    "- **tensorflow**: Framework de deep learning de Google\n",
    "- **keras**: API de alto nivel dentro de TensorFlow para construir modelos de redes neuronales\n",
    "  - layers: Capas para construir la arquitectura del modelo\n",
    "  - ImageDataGenerator: Generación y augmentación de imágenes\n",
    "  - callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint para entrenamiento avanzado\n",
    "\n",
    "#### Librerías de Visión por Computadora\n",
    "- **cv2 (OpenCV)**: Captura de video, detección de rostros, procesamiento de imágenes\n",
    "- **load_model**: Carga de modelos pre-entrenados\n",
    "\n",
    "Estas librerías son el **fundamento** para todo el sistema: desde el entrenamiento del modelo CNN hasta la detección en tiempo real con la cámara web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ab9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5829b",
   "metadata": {},
   "source": [
    "### Detección de Emociones en Tiempo Real (Versión Básica)\n",
    "\n",
    "La **Celda 4** implementa un sistema básico de detección de emociones usando la cámara web. Funciona en un hilo separado para no bloquear la interfaz.\n",
    "\n",
    "#### Carga del Modelo\n",
    "- Carga el modelo CNN pre-entrenado desde el archivo `best_emotion_model.h5`\n",
    "- Detecta 7 emociones: angry, disgust, fear, happy, neutral, sad, surprise\n",
    "\n",
    "#### Gestión de Assets\n",
    "- Carga imágenes PNG para cada emoción (en la carpeta Assets/)\n",
    "- Mapea cada emoción a un color BGR específico (formato OpenCV)\n",
    "- Asigna colores para visualización: rojo (cabreado), naranja (asqueado), azul (asustado), etc.\n",
    "\n",
    "#### Funciones de Visualización\n",
    "- `overlay_png_on_frame()`: Superpone imágenes PNG sobre rostros detectados con soporte para transparencia (canal alfa)\n",
    "- `add_emotion_effects()`: Agrega efectos visuales según la emoción (bordes de color, círculos decorativos, texto)\n",
    "\n",
    "#### Loop Principal de Detección\n",
    "- Captura video de la webcam en resolución 640x480\n",
    "- Convierte cada frame a escala de grises para detectar rostros\n",
    "- Para cada rostro detectado:\n",
    "  - Extrae la región y la redimensiona a 48x48 píxeles (formato del modelo)\n",
    "  - Normaliza la imagen y realiza predicción con el modelo CNN\n",
    "  - Aplica efectos visuales según la emoción detectada\n",
    "\n",
    "#### Control\n",
    "- Presionar 'q' para salir del programa\n",
    "- Timeout de 5 minutos si no se cierra manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cb849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventana abierta. Presiona 'q' para salir.\n",
      "Las emociones cambiarán el color y los efectos visuales\n",
      "\n",
      "Deteccion finalizada\n",
      "Sistema de detección de emociones finalizado\n",
      "Deteccion finalizada\n",
      "Sistema de detección de emociones finalizado\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "model = load_model('best_emotion_model.h5')\n",
    "\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "emotions_spanish = ['Cabreado', 'Asqueado', 'Asustado', 'Feliz', 'Neutral', 'Triste', 'Sorprendido']\n",
    "\n",
    "png_files = {\n",
    "    0: 'Assets/angry.jpg',\n",
    "    1: 'Assets/disgust.png',\n",
    "    2: 'Assets/fear.png',\n",
    "    3: 'Assets/happy.png',\n",
    "    4: 'Assets/neutral.png',\n",
    "    5: 'Assets/sadness.png',\n",
    "    6: 'Assets/surprise.png'\n",
    "}\n",
    "\n",
    "png_images = {}\n",
    "for idx, fname in png_files.items():\n",
    "    if os.path.exists(fname):\n",
    "        img = cv2.imread(fname, cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            print(f\"Advertencia: no se pudo leer {fname}\")\n",
    "        else:\n",
    "            png_images[idx] = img\n",
    "    else:\n",
    "        print(f\"Advertencia: archivo PNG no encontrado: {fname}\")\n",
    "\n",
    "emotion_config = {\n",
    "    0: {'color': (0, 0, 255), 'name': 'Cabreado'},\n",
    "    1: {'color': (0, 165, 255), 'name': 'Asqueado'},\n",
    "    2: {'color': (255, 0, 0), 'name': 'Asustado'},\n",
    "    3: {'color': (0, 255, 0), 'name': 'Feliz'},\n",
    "    4: {'color': (128, 128, 128), 'name': 'Neutral'},\n",
    "    5: {'color': (255, 0, 255), 'name': 'Triste'},\n",
    "    6: {'color': (0, 255, 255), 'name': 'Sorprendido'}\n",
    "}\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    ")\n",
    "\n",
    "stop_event = threading.Event()\n",
    "\n",
    "\n",
    "def overlay_png_on_frame(frame, png, x, y, w, h, scale=1.0):\n",
    "    if png is None:\n",
    "        return frame\n",
    "\n",
    "    target_w = int(w * scale)\n",
    "    target_h = int(h * scale)\n",
    "\n",
    "    png_h, png_w = png.shape[:2]\n",
    "    if png_w == 0 or png_h == 0:\n",
    "        return frame\n",
    "\n",
    "    resized = cv2.resize(png, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    cx = x + w // 2\n",
    "    cy = y + h // 2\n",
    "    x1 = int(cx - target_w // 2)\n",
    "    y1 = int(cy - target_h // 2)\n",
    "    x2 = x1 + target_w\n",
    "    y2 = y1 + target_h\n",
    "\n",
    "    fh, fw = frame.shape[:2]\n",
    "    ox1, oy1 = max(0, x1), max(0, y1)\n",
    "    ox2, oy2 = min(fw, x2), min(fh, y2)\n",
    "\n",
    "    if ox1 >= ox2 or oy1 >= oy2:\n",
    "        return frame\n",
    "\n",
    "    rx1 = ox1 - x1\n",
    "    ry1 = oy1 - y1\n",
    "    rx2 = rx1 + (ox2 - ox1)\n",
    "    ry2 = ry1 + (oy2 - oy1)\n",
    "\n",
    "    roi_frame = frame[oy1:oy2, ox1:ox2]\n",
    "    roi_png = resized[ry1:ry2, rx1:rx2]\n",
    "\n",
    "    if roi_png.shape[2] == 4:\n",
    "        b, g, r, a = cv2.split(roi_png)\n",
    "        alpha = a.astype(float) / 255.0\n",
    "        alpha = cv2.merge([alpha, alpha, alpha])\n",
    "        foreground = cv2.merge([b, g, r]).astype(float)\n",
    "        background = roi_frame.astype(float)\n",
    "        blended = cv2.convertScaleAbs(foreground * alpha + background * (1 - alpha))\n",
    "    else:\n",
    "        blended = roi_png[:, :, :3]\n",
    "\n",
    "    frame[oy1:oy2, ox1:ox2] = blended\n",
    "    return frame\n",
    "\n",
    "\n",
    "def add_emotion_effects(frame, emotion_idx, x, y, w, h, confidence):\n",
    "    config = emotion_config.get(emotion_idx, {'color': (255, 255, 255), 'name': str(emotion_idx)})\n",
    "\n",
    "    png = png_images.get(emotion_idx)\n",
    "    if png is not None:\n",
    "        scale = 1.2\n",
    "        frame = overlay_png_on_frame(frame, png, x, y, w, h, scale=scale)\n",
    "\n",
    "        text = f\"{config['name']}: {confidence:.2%}\"\n",
    "        cv2.putText(frame, text, (x + 5, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        return frame\n",
    "\n",
    "    color = config['color']\n",
    "\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (x - 5, y - 50), (x + 200, y - 10), color, -1)\n",
    "    cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n",
    "\n",
    "    text = f\"{config['name']}: {confidence:.2%}\"\n",
    "    cv2.putText(frame, text, (x + 5, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.circle(frame, (x + w//2, y + h//2), w//2 + 10, color, 2)\n",
    "\n",
    "    for corner_x, corner_y in [(x, y), (x+w, y), (x, y+h), (x+w, y+h)]:\n",
    "        cv2.circle(frame, (corner_x, corner_y), 5, color, -1)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def emotion_detection_thread():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    print(\"Ventana abierta. Presiona 'q' para salir.\")\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Error al capturar la cámara\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.3,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "\n",
    "        emotion_detected = None\n",
    "        max_confidence = 0\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "\n",
    "            roi_resized = cv2.resize(roi_gray, (48, 48))\n",
    "\n",
    "            roi_normalized = roi_resized / 255.0\n",
    "\n",
    "            roi_input = np.expand_dims(roi_normalized, axis=-1)\n",
    "            roi_input = np.expand_dims(roi_input, axis=0)\n",
    "\n",
    "            prediction = model.predict(roi_input, verbose=0)\n",
    "            emotion_idx = np.argmax(prediction)\n",
    "            confidence = prediction[0][emotion_idx]\n",
    "\n",
    "            if confidence > max_confidence:\n",
    "                max_confidence = confidence\n",
    "                emotion_detected = emotion_idx\n",
    "\n",
    "            frame = add_emotion_effects(frame, emotion_idx, x, y, w, h, confidence)\n",
    "\n",
    "        if emotion_detected is not None:\n",
    "            config = emotion_config[emotion_detected]\n",
    "            info_text = f\"Emocion Principal: {config['name']}\"\n",
    "            cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, config['color'], 2)\n",
    "\n",
    "            overlay = frame.copy()\n",
    "            color = config['color']\n",
    "            cv2.rectangle(overlay, (0, 0), (frame.shape[1], 60), color, -1)\n",
    "            cv2.addWeighted(overlay, 0.1, frame, 0.9, 0, frame)\n",
    "\n",
    "        cv2.imshow('Deteccion de Emociones', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            stop_event.set()\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "thread = threading.Thread(target=emotion_detection_thread, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "thread.join(timeout=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f5328",
   "metadata": {},
   "source": [
    "### Detección de Emociones en Tiempo Real (Versión Avanzada con Multimedia)\n",
    "\n",
    "La **Celda 5** implementa el sistema completo de reconocimiento de emociones con feedback multimedia, máquina de estados y control de alarmas.\n",
    "\n",
    "#### Importaciones y Configuración Inicial\n",
    "- Importa simpleaudio para reproducción de audio WAV con control de bucles\n",
    "- Carga el modelo CNN pre-entrenado\n",
    "- Define configuración de colores y nombres de emociones en español\n",
    "- Carga Haar Cascade para detección de rostros\n",
    "\n",
    "#### Búsqueda de Assets\n",
    "- `find_file()`: Función recursiva que busca archivos de audio e imágenes en el árbol de directorios\n",
    "- Busca archivos de audio: `SmokeAlarmChirp.wav` (sonido aleatorio), `AngerAlarm.wav` (alarma de cabreado)\n",
    "- Busca imagen: `DangerTriangle.png` (triángulo de advertencia, redimensionado a 200x200)\n",
    "\n",
    "#### Funciones de Audio\n",
    "- `play_random_smoke_alarm()`: Reproduce sonido aleatorio en hilo separado cada 5-10 segundos\n",
    "- `play_anger_alarm()`: Reproduce la alarma en bucle usando simpleaudio mientras `anger_alarm_playing` sea True\n",
    "- `stop_anger_alarm_now()`: Detiene inmediatamente la reproducción de audio llamando al método `.stop()`\n",
    "\n",
    "#### Visualización Multimedia\n",
    "- `overlay_danger_triangle()`: Superpone el triángulo centrado con soporte para transparencia (canal alfa RGBA)\n",
    "- Triángulo parpadea cada 0.5 segundos cuando está activo el estado de cabreado\n",
    "\n",
    "#### Máquina de Estados - Lógica Principal\n",
    "La detección funciona con dos estados principales:\n",
    "\n",
    "**Estado Normal**:\n",
    "- Detecta emociones y aplica overlay de color a toda la pantalla (20% opacidad)\n",
    "- Muestra nombre y confianza de la emoción detectada\n",
    "- Reproduce sonidos aleatorios cada 5-10 segundos\n",
    "\n",
    "**Estado Cabreado (angry)**:\n",
    "- Activado cuando se detecta emoción 0 (cabreado)\n",
    "- Inicia temporizador de 5 segundos\n",
    "- Cubre pantalla con overlay rojo (40% opacidad)\n",
    "- Muestra triángulo de peligro parpadeante\n",
    "- Reproduce alarma en bucle continuo\n",
    "- Decrementa contador visual cada segundo\n",
    "\n",
    "#### Transiciones de Estado\n",
    "- **Normal → Cabreado**: Cuando `emotion_detected == 0`\n",
    "- **Cabreado → Normal**: Si se detecta feliz (emoción 3) dentro de 5 segundos, se detiene alarma y se reinicia\n",
    "- **Cabreado → Fin**: Si pasan 5 segundos sin sonreír, se detiene alarma y cierra aplicación\n",
    "\n",
    "#### Variables Globales de Control\n",
    "- `anger_alarm_playing`: Flag booleano para controlar bucle de alarma\n",
    "- `anger_alarm_play_obj`: Referencia al objeto de reproducción para detención inmediata\n",
    "- `cabreado_active`: Indica si el sistema está en estado de cabreado\n",
    "- `danger_triangle_visible`: Controla visibilidad parpadeante del triángulo\n",
    "\n",
    "#### Thread Principal\n",
    "- `emotion_detection_thread()`: Ejecuta en hilo daemon la lógica principal\n",
    "- Captura video 640x480, procesa frames en tiempo real\n",
    "- Mantiene bucle activo hasta presionar 'q' o timeout de 5 segundos en cabreado\n",
    "- Limpia recursos: libera cámara, cierra ventanas, detiene alarma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29098360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ventana abierta. Pulsa 'q' para salir.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import simpleaudio as sa\n",
    "\n",
    "model = load_model('best_emotion_model.h5')\n",
    "\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "emotion_config = {\n",
    "    0: {'color': (0, 0, 255), 'name': 'Cabreado'},\n",
    "    1: {'color': (0, 165, 255), 'name': 'Asqueado'},\n",
    "    2: {'color': (255, 0, 0), 'name': 'Asustado'},\n",
    "    3: {'color': (0, 255, 0), 'name': 'Feliz'},\n",
    "    4: {'color': (128, 128, 128), 'name': 'Neutral'},\n",
    "    5: {'color': (255, 0, 255), 'name': 'Triste'},\n",
    "    6: {'color': (0, 255, 255), 'name': 'Sorprendido'}\n",
    "}\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "stop_event = threading.Event()\n",
    "\n",
    "def find_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return os.path.abspath(filename)\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        if filename in files:\n",
    "            return os.path.abspath(os.path.join(root, filename))\n",
    "    return None\n",
    "\n",
    "smoke_alarm_path = find_file('Assets/SmokeAlarmChirp.wav')\n",
    "anger_alarm_path = find_file('Assets/AngerAlarm.wav')\n",
    "\n",
    "danger_triangle_path = find_file('Assets/DangerTriangle.png')\n",
    "danger_triangle = None\n",
    "\n",
    "if danger_triangle_path:\n",
    "    danger_triangle = cv2.imread(danger_triangle_path, cv2.IMREAD_UNCHANGED)\n",
    "    if danger_triangle is not None:\n",
    "        danger_triangle = cv2.resize(danger_triangle, (200, 200), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        print(\"Error cargando DangerTriangle.png\")\n",
    "else:\n",
    "    print(\"DangerTriangle.png NO encontrado\")\n",
    "\n",
    "last_random_sound_time = 0\n",
    "anger_alarm_playing = False\n",
    "danger_triangle_visible = False\n",
    "danger_triangle_timer = 0\n",
    "anger_alarm_play_obj = None\n",
    "\n",
    "\n",
    "def play_random_smoke_alarm():\n",
    "    if not smoke_alarm_path:\n",
    "        return\n",
    "\n",
    "    def run():\n",
    "        try:\n",
    "            sound = sa.WaveObject.from_wave_file(smoke_alarm_path)\n",
    "            sound.play()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reproduciendo SmokeAlarmChirp.wav: {e}\")\n",
    "\n",
    "    threading.Thread(target=run, daemon=True).start()\n",
    "\n",
    "\n",
    "def play_anger_alarm():\n",
    "    global anger_alarm_play_obj, anger_alarm_playing\n",
    "\n",
    "    if not anger_alarm_path:\n",
    "        return\n",
    "\n",
    "    def loop():\n",
    "        global anger_alarm_play_obj\n",
    "        sound = sa.WaveObject.from_wave_file(anger_alarm_path)\n",
    "\n",
    "        while anger_alarm_playing and not stop_event.is_set():\n",
    "            anger_alarm_play_obj = sound.play()\n",
    "\n",
    "            while anger_alarm_play_obj.is_playing():\n",
    "                if not anger_alarm_playing or stop_event.is_set():\n",
    "                    break\n",
    "                time.sleep(0.05)\n",
    "\n",
    "    threading.Thread(target=loop, daemon=True).start()\n",
    "\n",
    "\n",
    "def stop_anger_alarm_now():\n",
    "    global anger_alarm_play_obj\n",
    "    try:\n",
    "        if anger_alarm_play_obj is not None:\n",
    "            anger_alarm_play_obj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def overlay_danger_triangle(frame, danger_triangle):\n",
    "    if danger_triangle is None:\n",
    "        return frame\n",
    "    \n",
    "    h, w = frame.shape[:2]\n",
    "    tri_h, tri_w = danger_triangle.shape[:2]\n",
    "    x = (w - tri_w) // 2\n",
    "    y = (h - tri_h) // 2\n",
    "    roi = frame[y:y + tri_h, x:x + tri_w]\n",
    "\n",
    "    if danger_triangle.shape[2] == 4:\n",
    "        b, g, r, a = cv2.split(danger_triangle)\n",
    "        alpha = a.astype(float) / 255.0\n",
    "        alpha = cv2.merge([alpha, alpha, alpha])\n",
    "        blended = cv2.convertScaleAbs(cv2.merge([b, g, r]).astype(float) * alpha + roi.astype(float) * (1 - alpha))\n",
    "        frame[y:y + tri_h, x:x + tri_w] = blended\n",
    "    else:\n",
    "        frame[y:y + tri_h, x:x + tri_w] = danger_triangle[:, :, :3]\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def emotion_detection_thread():\n",
    "    global last_random_sound_time, anger_alarm_playing, danger_triangle_visible, danger_triangle_timer\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    cabreado_timer = None\n",
    "    cabreado_active = False\n",
    "\n",
    "    print(\"\\nVentana abierta. Pulsa 'q' para salir.\\n\")\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        current_time = time.time()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5, minSize=(30, 30))\n",
    "\n",
    "        emotion_detected = None\n",
    "        max_confidence = 0\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_resized = cv2.resize(roi_gray, (48, 48))\n",
    "            roi_input = np.expand_dims(np.expand_dims(roi_resized / 255.0, -1), 0)\n",
    "\n",
    "            pred = model.predict(roi_input, verbose=0)\n",
    "            idx = np.argmax(pred)\n",
    "            conf = pred[0][idx]\n",
    "\n",
    "            if conf > max_confidence:\n",
    "                max_confidence = conf\n",
    "                emotion_detected = idx\n",
    "\n",
    "        if current_time - last_random_sound_time > random.uniform(5, 10):\n",
    "            if random.random() > 0.7:\n",
    "                play_random_smoke_alarm()\n",
    "            last_random_sound_time = current_time\n",
    "\n",
    "        if cabreado_active:\n",
    "\n",
    "            overlay = frame.copy()\n",
    "            overlay[:] = np.array(emotion_config[0]['color'], np.uint8)\n",
    "            cv2.addWeighted(overlay, 0.4, frame, 0.6, 0, frame)\n",
    "\n",
    "            if current_time - danger_triangle_timer > 0.5:\n",
    "                danger_triangle_visible = not danger_triangle_visible\n",
    "                danger_triangle_timer = current_time\n",
    "            \n",
    "            if danger_triangle_visible:\n",
    "                frame = overlay_danger_triangle(frame, danger_triangle)\n",
    "\n",
    "            remaining = 5 - (current_time - cabreado_timer)\n",
    "            if remaining > 0:\n",
    "                cv2.putText(frame, f\"Mosqueo detectado! Te quedan {int(remaining)} seg\",\n",
    "                            (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "                if emotion_detected == 3:\n",
    "                    cabreado_active = False\n",
    "                    anger_alarm_playing = False\n",
    "                    stop_anger_alarm_now()\n",
    "\n",
    "            else:\n",
    "                anger_alarm_playing = False\n",
    "                stop_anger_alarm_now()\n",
    "                stop_event.set()\n",
    "\n",
    "        else:\n",
    "            if emotion_detected == 0:\n",
    "                cabreado_active = True\n",
    "                cabreado_timer = current_time\n",
    "                danger_triangle_timer = current_time\n",
    "                danger_triangle_visible = True\n",
    "                if not anger_alarm_playing:\n",
    "                    anger_alarm_playing = True\n",
    "                    play_anger_alarm()\n",
    "\n",
    "            else:\n",
    "                if emotion_detected is not None:\n",
    "                    color = np.array(emotion_config[emotion_detected]['color'], np.uint8)\n",
    "                    overlay = frame.copy()\n",
    "                    overlay[:] = color\n",
    "                    cv2.addWeighted(overlay, 0.2, frame, 0.8, 0, frame)\n",
    "\n",
    "                    cv2.putText(frame,\n",
    "                                f\"{emotion_config[emotion_detected]['name']} ({max_confidence:.0%})\",\n",
    "                                (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                1, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow('Deteccion de Emociones', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            anger_alarm_playing = False\n",
    "            stop_anger_alarm_now()\n",
    "            stop_event.set()\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    anger_alarm_playing = False\n",
    "    stop_anger_alarm_now()\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=emotion_detection_thread, daemon=True)\n",
    "thread.start()\n",
    "thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
